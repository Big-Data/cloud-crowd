# The URL where you're planning on running the server/queue/database.
:central_server:          http://localhost:9173

# Please provide your AWS credentials for S3 storage of job output.
:aws_access_key:          [your AWS access key]
:aws_secret_key:          [your AWS secret access key]

# Choose an S3 bucket to store all CloudCrowd output, and decide if you'd like
# to keep all resulting files on S3 private. If so, you'll receive authenticated
# S3 URLs as job output, good for 24 hours. If left public, you'll get the
# straight URLs to the files on S3.
:s3_bucket:               [your CloudCrowd bucket]
:use_s3_authentication:   no

# Use HTTP Basic Auth for all requests? (Includes all internal worker requests 
# to the central server). If yes, specify the login and password that all 
# requests must provide for authentication.
:use_http_authentication: no
:login:                   [your login name]
:password:                [your password]

# Set the following numbers to tweak the configuration of your worker daemons. 
# Optimum results will depend on proportion of the Memory/CPU/IO bottlenecks
# in your actions, the number of central servers you have running, and your
# desired balance between latency and traffic.
  
# The number of workers that `crowd workers start` spins up.
:num_workers:             4

# The minimum number of seconds a worker waits between checking the job queue.
:min_worker_wait:         1

# The maximum number of seconds a worker waits between checking the job queue.
:max_worker_wait:         20

# The backoff multiplier the worker uses to slow down the check interval when 
# there's no work in the queue.
:worker_wait_multiplier:  1.3

# The number of seconds a worker waits to retry when there's some kind of 
# internal error (ie. the central server fails to respond)
:worker_retry_wait:       5

# The number of separate attempts that will be made to process an individual
# work unit, before marking it as having failed.
:work_unit_retries:       3
